{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04adf8e5-378d-4f57-8716-7360b2cc2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23917f56-d279-40ec-b420-0494b3d667de",
   "metadata": {},
   "source": [
    "## Step 1: Use NewsAPI to scrape headlines & articles URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86989da9-9ac3-429f-82a9-6b2a65d68c0f",
   "metadata": {},
   "source": [
    "News API is a simple HTTP REST API for searching and retrieving live articles from all over the web.?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0688c-04c2-467d-b7d5-f06c4be6a945",
   "metadata": {},
   "source": [
    "Using the FREE NewsAPI Developer Plan:\\\n",
    "‚úî 100 requests per day\\\n",
    "‚úî No extra request available\\\n",
    "‚úî Articles have a 24 hour delay\\\n",
    "‚úî Search articles up to a month old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4132e01-b9df-4f75-91bc-a016dc9754fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f76c35ca-71a9-493e-a856-26a878fd5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unofficial Python client library to integrate News API into Python application w/o having to make HTTP requests directly\n",
    "\n",
    "from newsapi import NewsApiClient # import newsapi client to set up NewsAPI object that will handle the req\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cff394-3d5f-4b9b-aa2b-99a676bf3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with NewsAPI key\n",
    "API_KEY = '37439fc0e11546dd9b81a6f698800573'\n",
    "newsapi = NewsApiClient(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6dec4e-7b92-4b3f-82d7-0116e3d0cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap weekly till today\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30) # since can only search articles up to 1 month old \n",
    "date_ranges = pd.date_range(start=start_date, end=end_date, freq = '7D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff11b22-7fca-4e98-b408-76a4baf7bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores scraped URLs, sources and titles\n",
    "all_sources = []\n",
    "all_URLs = []\n",
    "all_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a3ab7fe-74fc-44ee-a7d8-c0e654e28991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory and file paths\n",
    "save_dir = r\"C:\\Users\\jiayi\\OneDrive - National University of Singapore\\Desktop\\DSA3101\"\n",
    "csv_file = os.path.join(save_dir, \"esg_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80225b18-74c0-4e9f-b64e-aec9e61eb583",
   "metadata": {},
   "source": [
    "#### Using Endpoints listed on NewsAPI documentation\n",
    "3 methods to fetch data with API object\\\n",
    "‚úî get_top_headlines( )\\\n",
    "‚úî get_everything( )\\\n",
    "‚úî get_sources( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ac6e81-e413-41b7-bd19-591b5eb8ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping URLs from NewsAPI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API failed on 2025-02-04 to 2025-02-11: {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2025-02-05, but you have requested 2025-02-04. You may need to upgrade to a paid plan.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:30<00:00,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total URLs Scraped: 267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Scraping URLs from NewsAPI...\")\n",
    "\n",
    "# loop through each week to scrape URLs and titles \n",
    "for i in tqdm(range(len(date_ranges) -1)):\n",
    "    from_date = date_ranges[i].strftime(\"%Y-%m-%d\")\n",
    "    to_date = date_ranges[i + 1].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    try:\n",
    "        data = newsapi.get_everything(q = 'esg', # phrase to search in article title/ body\n",
    "                                      from_param = from_date,\n",
    "                                      to = to_date, \n",
    "                                      language = 'en', # only articles in eng\n",
    "                                      page_size = 100 # number of results to return per page\n",
    "                                     )\n",
    "\n",
    "        articles = data.get(\"articles\", [])\n",
    "        for article in articles:\n",
    "            if article[\"url\"] not in all_URLs:\n",
    "                all_URLs.append(article[\"url\"])\n",
    "                all_titles.append(article[\"title\"])\n",
    "                all_sources.append(article[\"source\"])\n",
    "\n",
    "        sleep(random.uniform(3, 8))  # random sleep time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"API failed on {from_date} to {to_date}: {e}\")\n",
    "        sleep(10)  # sleep before retrying\n",
    "\n",
    "print(f\" Total URLs Scraped: {len(all_URLs)}\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9517bcd-da4e-4d5b-8c78-c10aaad8c210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': {'id': None, 'name': 'Forbes'},\n",
       " 'author': 'Rajeev Peshawaria, Contributor, \\n Rajeev Peshawaria, Contributor\\n https://www.forbes.com/sites/rajeevpeshawaria/',\n",
       " 'title': 'Five Reasons Why ESG And Wokeism Are Taking A Beating',\n",
       " 'description': 'My simple assertion, one we‚Äôve been making for a long time now, is that we were trying to solve existential environmental and social challenges unnaturally.',\n",
       " 'url': 'https://www.forbes.com/sites/rajeevpeshawaria/2025/03/04/five-reasons-why-esg-and-wokeism-are-taking-a-beating/',\n",
       " 'urlToImage': 'https://imageio.forbes.com/specials-images/imageserve/67c6bcdb4412ddedfe99df4c/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds',\n",
       " 'publishedAt': '2025-03-04T08:36:04Z',\n",
       " 'content': 'The dearth of ESG and DEI?\\r\\nImage credit: ChatGPT\\r\\nWere ESG, DEI and sustainability fads that have since died? It certainly feels like it in some circles, doesnt it? I dont believe they are dead at a‚Ä¶ [+5040 chars]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first article in list\n",
    "data['articles'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e3939-3872-45a4-88a6-2541c611969e",
   "metadata": {},
   "source": [
    "## Step 2: Use BeautifulSoup to visit and scrape all contents from each URL\n",
    "\n",
    "BeautifulSoup = python library to pull out data from HTML and XML files.\\\n",
    "Automatically visit each article URL to scrape full content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a2ed17-515f-4752-abea-6943c4fe84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ec10e3e-51cf-44df-819f-073f5d9d87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Scraping Full News Content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 267/267 [16:35<00:00,  3.73s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Scraping Full News Content...\")\n",
    "\n",
    "# getting full text content from scraped articles URL\n",
    "\n",
    "text = [] \n",
    "for url in tqdm(all_URLs):\n",
    "    try:\n",
    "        response = requests.get(url)  \n",
    "        if response.status_code != 200: # req unsuccessful\n",
    "            text.append(\"Unable to scrape text\")  # fill missing text\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # try different HTML patterns\n",
    "        content = soup.find(\"div\", {\"id\": re.compile(\"^content-body-[0-9]+\")})\n",
    "        if not content:\n",
    "            content = soup.find(\"article\")\n",
    "        if not content:\n",
    "            content = soup.find(\"div\", {\"class\": re.compile(\".*content.*\")})\n",
    "        if not content:\n",
    "            content = soup.find(\"p\")  # fallback to paragraphs\n",
    "\n",
    "        if content:\n",
    "            text.append(content.get_text(strip=True))\n",
    "        else:\n",
    "            text.append(\"Content Not Found\")  # HTML patterns failed\n",
    "\n",
    "        sleep(random.uniform(2, 5))  # sleep to avoid detection\n",
    "\n",
    "    except Exception as e:\n",
    "        text.append(\"Failed to Scrape\")  # for any unknown exception\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6351fc0c-dd7d-459a-a988-823cb69346b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "267\n",
      "267\n",
      "267\n"
     ]
    }
   ],
   "source": [
    "# ensure that all 4 lists have the same length before saving to CSV\n",
    "print(len(all_URLs))\n",
    "print(len(all_titles))\n",
    "print(len(all_sources))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21dd265d-3579-481e-9a90-aa9018ca8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® No Existing CSV Found... Creating New CSV\n",
      "‚úÖ Data Saved to C:\\Users\\jiayi\\OneDrive - National University of Singapore\\Desktop\\DSA3101\\esg_articles.csv\n"
     ]
    }
   ],
   "source": [
    "# create a df with the scraped and cleaned data\n",
    "scraped_text_df = pd.DataFrame({\n",
    "    \"Title\": all_titles,\n",
    "    \"Source\": all_sources,\n",
    "    \"URL\": all_URLs,\n",
    "    \"Content\": text,\n",
    "    })\n",
    "\n",
    "# if the CSV file exists, append new data\n",
    "if os.path.exists(csv_file):\n",
    "    print(\"‚úÖ Existing CSV Found. Appending Data...\")\n",
    "    existing_df = pd.read_csv(csv_file)\n",
    "    scraped_text_df = pd.concat([existing_df, scraped_text_df], ignore_index=True)\n",
    "    scraped_text_df = scraped_text_df.drop_duplicates(subset=\"URL\", keep=\"first\")\n",
    "    print(f\"{len(scraped_text_df) - len(existing_df)} New Articles Appended\")\n",
    "else:\n",
    "    print(\"üö® No Existing CSV Found... Creating New CSV\")\n",
    "\n",
    "# save scraped data into the CSV file\n",
    "scraped_text_df.to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úÖ Data Saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b0ec441-bb14-4b2d-a908-b6872d9cc202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How TIME and Statista Determined the World‚Äôs B...</td>\n",
       "      <td>{'id': 'time', 'name': 'Time'}</td>\n",
       "      <td>https://time.com/7221214/worlds-best-companies...</td>\n",
       "      <td>ByTIME StaffFebruary 12, 2025 7:48 AM ESTTIME ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How VCs are killing climate tech ‚Äî and how the...</td>\n",
       "      <td>{'id': 'the-next-web', 'name': 'The Next Web'}</td>\n",
       "      <td>https://thenextweb.com/news/how-vcs-are-killin...</td>\n",
       "      <td>Unable to scrape text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Costco's DEI clash has companies taking notes....</td>\n",
       "      <td>{'id': 'business-insider', 'name': 'Business I...</td>\n",
       "      <td>https://www.businessinsider.com/dei-costco-dis...</td>\n",
       "      <td>RetailCostco's DEI clash has companies taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Great Wealth Transfer: Managing Inheritanc...</td>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>https://www.forbes.com/sites/matthewerskine/20...</td>\n",
       "      <td>MoneyWealth ManagementThe Great Wealth Transfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juventus FC Looks To Become Serie A‚Äôs Most Sus...</td>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>https://www.forbes.com/sites/vitascarosella/20...</td>\n",
       "      <td>BusinessSportsMoneyJuventus FC Looks To Become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Diginex Limited Announces Relocation of Headqu...</td>\n",
       "      <td>{'id': None, 'name': 'GlobeNewswire'}</td>\n",
       "      <td>https://www.globenewswire.com/news-release/202...</td>\n",
       "      <td>Diginex Limited Announces Relocation of Headqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Can Microreactors Solve Data Centers' Unsustai...</td>\n",
       "      <td>{'id': None, 'name': 'Storagereview.com'}</td>\n",
       "      <td>https://www.storagereview.com/news/can-microre...</td>\n",
       "      <td>AIEnterpriseCan Microreactors Solve Data Cente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Government fund ‚Äòcomfortable‚Äô with $32m harris...</td>\n",
       "      <td>{'id': None, 'name': 'Crikey'}</td>\n",
       "      <td>http://www.crikey.com.au/2025/02/28/government...</td>\n",
       "      <td>Share this articleIf you like this article, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Head to Head Review: Versus Systems (NASDAQ:VS...</td>\n",
       "      <td>{'id': None, 'name': 'ETF Daily News'}</td>\n",
       "      <td>https://www.etfdailynews.com/2025/02/26/head-t...</td>\n",
       "      <td>Head to Head Review: Versus Systems (NASDAQ:VS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Businesses praise, sustainability advocates bl...</td>\n",
       "      <td>{'id': None, 'name': 'EURACTIV'}</td>\n",
       "      <td>https://www.euractiv.com/section/eet/news/busi...</td>\n",
       "      <td>Unable to scrape text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    How TIME and Statista Determined the World‚Äôs B...   \n",
       "1    How VCs are killing climate tech ‚Äî and how the...   \n",
       "2    Costco's DEI clash has companies taking notes....   \n",
       "3    The Great Wealth Transfer: Managing Inheritanc...   \n",
       "4    Juventus FC Looks To Become Serie A‚Äôs Most Sus...   \n",
       "..                                                 ...   \n",
       "262  Diginex Limited Announces Relocation of Headqu...   \n",
       "263  Can Microreactors Solve Data Centers' Unsustai...   \n",
       "264  Government fund ‚Äòcomfortable‚Äô with $32m harris...   \n",
       "265  Head to Head Review: Versus Systems (NASDAQ:VS...   \n",
       "266  Businesses praise, sustainability advocates bl...   \n",
       "\n",
       "                                                Source  \\\n",
       "0                       {'id': 'time', 'name': 'Time'}   \n",
       "1       {'id': 'the-next-web', 'name': 'The Next Web'}   \n",
       "2    {'id': 'business-insider', 'name': 'Business I...   \n",
       "3                       {'id': None, 'name': 'Forbes'}   \n",
       "4                       {'id': None, 'name': 'Forbes'}   \n",
       "..                                                 ...   \n",
       "262              {'id': None, 'name': 'GlobeNewswire'}   \n",
       "263          {'id': None, 'name': 'Storagereview.com'}   \n",
       "264                     {'id': None, 'name': 'Crikey'}   \n",
       "265             {'id': None, 'name': 'ETF Daily News'}   \n",
       "266                   {'id': None, 'name': 'EURACTIV'}   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://time.com/7221214/worlds-best-companies...   \n",
       "1    https://thenextweb.com/news/how-vcs-are-killin...   \n",
       "2    https://www.businessinsider.com/dei-costco-dis...   \n",
       "3    https://www.forbes.com/sites/matthewerskine/20...   \n",
       "4    https://www.forbes.com/sites/vitascarosella/20...   \n",
       "..                                                 ...   \n",
       "262  https://www.globenewswire.com/news-release/202...   \n",
       "263  https://www.storagereview.com/news/can-microre...   \n",
       "264  http://www.crikey.com.au/2025/02/28/government...   \n",
       "265  https://www.etfdailynews.com/2025/02/26/head-t...   \n",
       "266  https://www.euractiv.com/section/eet/news/busi...   \n",
       "\n",
       "                                               Content  \n",
       "0    ByTIME StaffFebruary 12, 2025 7:48 AM ESTTIME ...  \n",
       "1                                Unable to scrape text  \n",
       "2    RetailCostco's DEI clash has companies taking ...  \n",
       "3    MoneyWealth ManagementThe Great Wealth Transfe...  \n",
       "4    BusinessSportsMoneyJuventus FC Looks To Become...  \n",
       "..                                                 ...  \n",
       "262  Diginex Limited Announces Relocation of Headqu...  \n",
       "263  AIEnterpriseCan Microreactors Solve Data Cente...  \n",
       "264  Share this articleIf you like this article, sh...  \n",
       "265  Head to Head Review: Versus Systems (NASDAQ:VS...  \n",
       "266                              Unable to scrape text  \n",
       "\n",
       "[267 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c4daa-2fba-4cc5-a903-dbd669d9f4d0",
   "metadata": {},
   "source": [
    "## Step 3: Text Splitting for Manageable Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "299d3890-6f27-4010-8e4c-ea182ca0083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a4f65d56-0e8d-451a-9d59-da3736d384b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2a02ded6-bb1a-4cd1-b3df-0b60d1182295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV into df\n",
    "df = pd.read_csv('esg_articles.csv')\n",
    "\n",
    "# initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200, chunk_overlap=100, add_start_index=True\n",
    ")\n",
    "\n",
    "# create a list to store the documents objects\n",
    "documents = []\n",
    "\n",
    "# loop the rows of the df and convert them into Document format\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # check if content is valid (not NaN or 'Unable to scrape text')\n",
    "    if pd.notna(row['Content']) and row['Content'] != 'Unable to scrape text':\n",
    "        document = Document(page_content=row['Content'], metadata={'title': row['Title'], 'source': row['Source'], 'url': row['URL']})\n",
    "        documents.append(document) # append to documents list\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents) # all_splits now contains the chunked text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "21235a28-b13b-41f8-873e-2be026383e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9233e3-fc5e-4b45-bf7e-4cec2f8ce622",
   "metadata": {},
   "source": [
    "## Step 4: Generating Embeddings and storing in Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ed0bfc01-6cd9-4bdc-837e-c2f39e7a12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "40116d7b-1340-466b-9bd5-8b02d8e6d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f7c02d50-9d7d-4463-b2af-775048f2fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOST = \"http://localhost:9200\"\n",
    "index_name = \"esg_articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "22ef5769-87e3-49f1-942b-39916e83adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sentence transformer for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # same model as ESG reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c61f273b-dedc-4111-88ee-9d2054530540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize local es\n",
    "try:\n",
    "    es = Elasticsearch([ES_HOST])\n",
    "except Exception as e:\n",
    "    raise Exception(\n",
    "        status_code=500, detail=f\"Failed to connect to Elasticsearch: {str(e)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e3217d75-89d0-4ea1-953e-e1fb0e14b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure index in elasticsearch is create w correct mapping - stored as dense_vector\n",
    "index_mapping = {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"source\": {\"type\": \"keyword\"},\n",
    "            \"url\": {\"type\": \"keyword\"},\n",
    "            \"content\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,  # The dimension of your embeddings, make sure this matches\n",
    "                \"similarity\" : \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8c9984fa-6aa5-4e7b-aeb3-f6d8ca34694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    try:\n",
    "        # Create embeddings and convert to list from as needed by Elasticsearch\n",
    "        return embedding_model.encode(text).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching embeddings for text: {text}. Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_index():\n",
    "    try:\n",
    "        # Delete index if it already exists\n",
    "        if es.indices.exists(index=index_name):\n",
    "            es.indices.delete(index=index_name)\n",
    "        # Create index with mapping\n",
    "        es.indices.create(index=index_name, mappings=index_mapping)\n",
    "        print(f\"Index '{index_name}' created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index '{index_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a137d59f-7deb-45c1-a2ec-6a3875b17fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing all_splits data into Elasticsearch\n",
    "def generate_documents_with_embeddings(all_splits):\n",
    "    for doc in all_splits:\n",
    "        action = {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"title\": doc.metadata['title'],\n",
    "                \"source\": doc.metadata['source'],\n",
    "                \"url\": doc.metadata['url'],\n",
    "                \"content\": doc.page_content,\n",
    "                \"embeddings\": embedding_model.encode(doc.page_content).tolist()\n",
    "            }\n",
    "        }\n",
    "        yield action  # Yield the action instead of just defining it\n",
    "\n",
    "def index_documents(all_splits):\n",
    "    success, failed = bulk(es, generate_documents_with_embeddings(all_splits))\n",
    "    print(f\"Successfully indexed {success} documents. Failed to index {failed} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f1eee983-d257-46ee-b807-758c1c8b6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'esg_articles' created successfully!\n",
      "Successfully indexed 1843 documents. Failed to index [] documents.\n"
     ]
    }
   ],
   "source": [
    "create_index()  # Ensure the index is created first\n",
    "index_documents(all_splits)  # Index the documents with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0c0e7302-7217-4256-89f7-4d19cf8c6b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'esg_articles': {'mappings': {'properties': {'content': {'type': 'text', 'analyzer': 'standard'}, 'embeddings': {'type': 'dense_vector', 'dims': 384, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'source': {'type': 'keyword'}, 'title': {'type': 'text', 'analyzer': 'standard'}, 'url': {'type': 'keyword'}}}}}\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.get_mapping(index=index_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "dec27586-1d69-47c7-ac32-df478585456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Beyond ESG: Why Innovation Is A Defining Factor For Board Leadership\n",
      "URL: https://www.forbes.com/councils/forbestechcouncil/2025/01/31/beyond-esg-why-innovation-is-a-defining-factor-for-board-leadership/\n",
      "Content: InnovationBeyond ESG: Why Innovation Is A Defining Factor For Board LeadershipByGreg Ombach, Forbes Councils Member.forForbes Technology CouncilCOUNCIL POSTExpertise from Forbes Councils members, operated under license. Opinions expressed are those of the author.| Membership (fee-based)Jan 31, 2025, 08:15am ESTSave ArticleGreg Ombach, Head of Disruptive Research & Technology, Senior Vice President atAirbus.gettyThroughout my career, I have worked in the automotive, consumer electronics, and aerospace industries and served on various boards and advisory committees ranging from startups to publicly listed enterprises. These experiences have shown me how corporate governance has evolved to address emerging challenges like AI and environmental, social and governance (ESG). Yet, as industries face rapid transformation, innovation has become the defining factor for companies striving to lead, adapt and create value in an increasingly competitive global landscape.While boards have progressed in embedding ESG principles and navigating technological disruption, I believe innovation remains under-prioritized. To ensure resilience and long-term growth, boards must embrace innovation as a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the documents (e.g., first 5 documents)\n",
    "index_name = \"esg_articles\"\n",
    "response = es.search(index=index_name, body={\n",
    "    \"size\": 1,  # Number of documents to retrieve\n",
    "    \"_source\": [\"title\", \"content\", \"url\", \"embeddings\"]  # Specify the fields to retrieve\n",
    "})\n",
    "\n",
    "# Print the full response to inspect all available fields\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(f\"URL: {hit['_source']['url']}\")\n",
    "    print(f\"Content: {hit['_source']['content']}\")\n",
    "    #print(f\"Embeddings: {hit['_source']['embeddings']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c71eeb-97af-4e23-8475-cf7c1c18213e",
   "metadata": {},
   "source": [
    "## Step 5: Implementing the Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0756c5-ea72-4c21-81e2-3d2dc3d2de93",
   "metadata": {},
   "source": [
    "Implement a KNN (nearest neighbor) search, retrieve the top K documents with similar document chunks based on embeddings from Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "349030e6-a66e-4c1b-927b-22bd94398829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(query):\n",
    "    return embedding_model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c493f181-b355-4b3d-8029-191b4cf237be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(query, index_name, k=5):\n",
    "    query_embedding = get_embeddings(query)\n",
    "\n",
    "    search_query = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"field\": \"embeddings\",  # ‚úÖ Use 'field' in ES 8.x\n",
    "                \"query_vector\": query_embedding,\n",
    "                \"k\": k,\n",
    "                \"num_candidates\": 100  # Pre-select top 100 documents\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"title\", \"url\", \"content\"]  # Only return these fields\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index_name, body=search_query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a7d0a1be-b91e-4c8a-959d-a7e3baf586a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Future Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event\n",
      "URL: https://www.hospitalitynet.org/news/4125661.html\n",
      "Similarity Score: 0.9050162\n",
      "\n",
      "Title: Future Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event\n",
      "URL: https://www.hospitalitynet.org/news/4125661.html\n",
      "Similarity Score: 0.8496554\n",
      "\n",
      "Title: Top takeaways from the FHS World Advisory Board\n",
      "URL: https://www.hospitalitynet.org/news/4125791.html\n",
      "Similarity Score: 0.84051085\n",
      "\n",
      "Title: Top takeaways from the FHS World Advisory Board\n",
      "URL: https://www.hospitalitynet.org/news/4125791.html\n",
      "Similarity Score: 0.8343766\n",
      "\n",
      "Title: Future Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event\n",
      "URL: https://www.hospitalitynet.org/news/4125661.html\n",
      "Similarity Score: 0.81534505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take top K most similar document chunks from Elasticsearch\n",
    "query = \"FHS World 2025 mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event.\"\n",
    "response = knn_search(query, \"esg_articles\")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(f\"URL: {hit['_source']['url']}\")\n",
    "    # print(f\"Content:{hit['_source']['content']}\")\n",
    "    print(f\"Similarity Score: {hit['_score']}\\n\") # score of how relevant/similar the content is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cdeea7b8-01eb-4cda-aae4-f087feec0fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Combined Page Content ###\n",
      "Press ReleaseEvents & ConferencesFuture Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment eventFHS World set to return to Madinat Jumeirah in Dubai from 27-29 October 2025 for landmark editionThe Bench4 February 2025Future Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event ‚Äî Source:The BenchFuture Hospitality Summit ‚Äì FHS World 2025 to mark 20 years in the UAE for the region‚Äôs most influential hospitality and tourism investment event ‚Äî Source:The BenchDubai, UAE. 4 February 2025. After another record event in 2024,Future Hospitality Summit‚Äì FHS World, will return to Dubai from 27-29 October 2025 for what will be a milestone event for organisersThe Bench, marking 20 years in the UAE for the region‚Äôs leading hospitality and tourism investment event, previously known as AHIC.FHS World 2024 saw a record 1,596 attendees representing 735 companies from 60 countries, of which over 550 were women faciliated by the organiser‚Äôs #FHSWomenPower initiative.TheFHS World Advisory Boardconvened in Dubai last week to discuss and\n",
      "\n",
      "future of hospitality investment, lifestyle hospitality & well-being, and hospitality spaces. In addition, there will be sessions on hospitality investment opportunities in Africa, Europe and Asia as part of the programme.Delegates can look forward to the usual strong debate, dealmaking and networking that FHS is known for, along with its many immersive features such as the TenX Leadership Talks, Roundtable Discussions, Country Pavilion Showcases, the Branded Residences Forum, Sustainable Hospitality Challenge student competition and ESG Lab.FHS is firmly established as the leading event of its kind in the region, with two growing, annual events that underpin the unprecedented expansion of the Middle East‚Äôs tourism offering. The Bench itself is also growing in line with FHS‚Äôs expansion. Olja Nicholl has joined as Head of Sales for FHS World, as part of the company‚Äôs multi-cultural, highly experienced team.This year, FHS Saudi Arabia takes place from 11-13 May at the Mandarin Oriental Al Faisaliah in Riyadh, followed by FHS World from 27-29 October 2025 at Madinat Jumeirah in Dubai.Registration is now open with Super Early Bird rate passes available\n",
      "\n",
      "Press ReleaseFinanceTop takeaways from the FHS World Advisory BoardAdvisory Board members offered a treasure trove of insights, recommendations, and strategic ideas aimed at shaping the upcoming event into an industry-defining gatheringThe Bench12 February 2025Top takeaways from the FHS World Advisory Board ‚Äî Source:The BenchTop takeaways from the FHS World Advisory Board ‚Äî Source:The BenchDubai, UAE- Over 45 senior hospitality leaders came together in Dubai for the FHS World Advisory Board Meeting recently to discuss and shape the vision and agenda for theFuture Hospitality Summit‚Äì FHS World, taking place in Dubai from 27-29 October 2025 at Madinat Jumeirah in Dubai.The Advisory Board meeting highlighted the fact that the hospitality industry is at a pivotal moment in time with critical trends, opportunities and challenges facing the industry today and in the future. Members expressed a clear desire for FHS World 2025 to address these with practical, actionable insights. The meeting provided a valuable roadmap for developing an engaging and impactful programme for the landmark 20thedition of the event.TheAdvisory Boardcomprises esteemed leaders from across the hospitality industry\n",
      "\n",
      "design strategies to combat climate challenges like wildfires and floods.Under the 2025 conference theme ‚ÄòInvest in our Future‚Äô, Future Hospitality Summit ‚Äì FHS World, will take place at Madinat Jumeirah in Dubai from 27-29 October 2025 for what will be a milestone event for organisers The Bench, marking 20 years in the UAE for the region‚Äôs leading hospitality and tourism investment event, previously known as AHIC.About The BenchThe Bench has built a legacy as a global curator of opportunity, designing transformative forums and summits that empower the hospitality and travel industries to connect, innovate, and thrive. With over two decades of expertise, The Bench creates platforms that go beyond transactions to inspire collaboration and drive meaningful change.Each event brings together government leaders, tourism ministries, global travel associations, leading hospitality brands, hotel owners and investors, airlines, destination developers, and more. From flagship gatherings like FHS World (formerly AHIC) and FHS Saudi Arabia, to FHS Africa (formerly AHIF) and AviaDev, The Bench creates events where ideas spark, relationships deepen, and investments take flight.The Bench thrives\n",
      "\n",
      "#FHSWomenPower initiative.TheFHS World Advisory Boardconvened in Dubai last week to discuss and shape the vision and agenda for this year‚Äôs event. The Advisory Board comprises some of the hospitality industry‚Äôs most prominent leaders, includingAnthony Costa, Head of Hospitality & Lifestyle, Candy Capital;Marc Dardenne, Chief Executive Officer, Modon Hospitality;Amit Arora, Chief Executive Officer, Arada;Dinky Puri, Founding Partner, Eagle Wing;Othmane Jabri, Principal, Real Estate and Hospitality (Investment), Investment Corporation of Dubai;Nicolas Mayer, Partner, Global Industry Leader Tourism, PwC;Philipp J. Klohr, Senior Vice President, Mubadala Investment Company;Jeff Tisdall, Chief Business Officer Accor One Living, Global Head of Mixed-Use,Ahmed Nazim, Managing Director, Maldives Fund Management Corporation;Aboudi Asali,Executive Vice President, Hotels & Hospitality MENA, JLL, andSaahil Lalit, Vice President | Lodging Development EMEA, Marriott International.Jonathan Worsley, Chairman of The Bench, said: ‚ÄúThis will be a very exciting year for us as we celebrate our 20thanniversary in the UAE, and at Madinat Jumeirah where it all began in 2005, and we‚Äôre thrilled to have\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine the most relevant content together\n",
    "def combine_content(response):\n",
    "    combined_content = \"\"\n",
    "    for hit in response['hits']['hits']:\n",
    "        content = hit[\"_source\"][\"content\"]\n",
    "        combined_content += content + \"\\n\\n\" # add line btw articles\n",
    "    return combined_content \n",
    "    \n",
    "combined_text = combine_content(response) \n",
    "print(\"### Combined Page Content ###\")\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067ee80-5bb2-42b6-a4c0-773b9d95e8f2",
   "metadata": {},
   "source": [
    "## Step 6: Generate Responses with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c0b62f0a-111b-4db5-aa63-f889510608a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "def fact_check(query, combined_text, confidence=False):\n",
    "    if not combined_text.strip():\n",
    "        return \"Not Conclusive (No relevant context found)\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert fact-checking assistant.\n",
    "\n",
    "    Fact: \"{query}\"\n",
    "    Context: {combined_text}\n",
    "\n",
    "    Compare the fact against the context.\n",
    "    Classify the fact as one of the following:\n",
    "    - \"Yes\": The context **confirms** the fact.\n",
    "    - \"No\": The context **contradicts** the fact.\n",
    "    - \"Not Conclusive\": The context does **not provide enough information** to confirm or contradict.\n",
    "\n",
    "    Only return the label as output.\n",
    "    \"\"\"\n",
    "\n",
    "    if confidence:\n",
    "        prompt += \"\\nAdditionally, provide a confidence score from 0 to 100 in parentheses, e.g., Same (95%).\"\n",
    "\n",
    "    response = llm.invoke(prompt).strip()\n",
    "\n",
    "    # Optional: Normalize the output\n",
    "    response = response.split(\"\\n\")[0]  # In case LLM gives extra text\n",
    "    response = response.capitalize()  # Force consistent casing\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8bf0b142-2a52-428a-ba17-4d8b2fe3ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \"yes\" (100%)\n"
     ]
    }
   ],
   "source": [
    "result = fact_check(query, combined_text, confidence=True)\n",
    "print(\"Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
